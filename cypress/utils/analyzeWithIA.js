// Carrega as vari√°veis de ambiente do .env (como chave da API)
require('dotenv').config();

// Importa dinamicamente o node-fetch (necess√°rio para compatibilidade com ESModules)
const fetch = (...args) =>
  import('node-fetch').then(({ default: fetch }) => fetch(...args));

// L√™ as vari√°veis de ambiente
const OPENROUTER_API_URL = process.env.OPENROUTER_API_URL;
const MODEL_NAME = 'meta-llama/llama-4-maverick:free'; // Modelo gratuito do OpenRouter

// Fun√ß√£o respons√°vel por enviar o erro para IA e retornar a sugest√£o personalizada
async function analyzeWithIA(logDeErro) {
  const prompt = `
Voc√™ √© um especialista em QA. Analise o seguinte erro de teste automatizado:

"${logDeErro}"

Responda com:
1. üìå **Motivo prov√°vel da falha**
2. ‚úÖ **Sugest√£o de corre√ß√£o**
`;

  try {
    const response = await fetch(OPENROUTER_API_URL, {
      method: 'POST',
      headers: {
        Authorization: `Bearer ${process.env.OPENROUTER_API_KEY}`, // Token de API (via .env)
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: MODEL_NAME,
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.3,
        max_tokens: 300
      })
    });

    const data = await response.json();
    return data.choices?.[0]?.message?.content || '‚ùå Erro: resposta da IA vazia.';
  } catch (error) {
    return `‚ùå Erro ao se comunicar com a IA: ${error.message}`;
  }
}

// Exporta a fun√ß√£o para uso em outros arquivos
module.exports = { analyzeWithIA };
